{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":8315705,"sourceType":"datasetVersion","datasetId":4939623},{"sourceId":8318193,"sourceType":"datasetVersion","datasetId":4940685}],"dockerImageVersionId":30699,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"from torch import nn\nimport torch\nimport pickle\ndevice = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\nprint(device)\nclass LSTM(nn.Module):\n\n    def __init__(self, input_dim, hidden_dim, num_layers, output_dim):\n        super(LSTM, self).__init__()\n        self.hidden_dim = hidden_dim\n        self.num_layers = num_layers\n        self.sigmoid = nn.Sigmoid()\n\n        self.lstm = nn.LSTM(input_dim, hidden_dim, num_layers, batch_first=True)\n        self.fc = nn.Linear(hidden_dim, output_dim)\n\n    def forward(self, x):\n        h0 = torch.zeros(self.num_layers, x.size(0), self.hidden_dim).requires_grad_().to(device)\n        c0 = torch.zeros(self.num_layers, x.size(0), self.hidden_dim).requires_grad_().to(device)\n        out, (hn, cn) = self.lstm(x, (h0.detach(), c0.detach()))\n        out = self.fc(out[:, -1, :])\n        return self.sigmoid(out)\n\ninput_dim = 10000\nhidden_dim = 64\nnum_layers = 4\noutput_dim = 1\n\ndef load_model(model_path, vectorizer_path):\n    model = LSTM(input_dim=input_dim, hidden_dim=hidden_dim, output_dim=output_dim, num_layers=num_layers)\n    model.load_state_dict(torch.load(model_path))\n    with open(vectorizer_path, 'rb') as f:\n        vectorizer = pickle.load(f)\n    return model, vectorizer\n\ndef transform_input(input_string, vectorizer, device):\n    input_vector = vectorizer.transform([input_string])\n    input_vector = input_vector.toarray()\n    input_vector = input_vector.reshape((input_vector.shape[0], 1, input_vector.shape[-1]))\n    input_tensor = torch.tensor(input_vector, dtype=torch.float32)\n    #input_tensor = input_tensor.unsqueeze(0)  # Add batch dimension\n    return input_tensor\n\ninput_string1 = \"Say to them,'If you don't stop plagiarizing my work, I'll have no qualms about exposing you for the fraud you are.'\"\ninput_string2 = \"Address the issue directly with your coworker by presenting evidence of the plagiarism and seek resolution.\"\ninput_string3 = \"Say to them, 'I won't tolerate your plagiarism. Shape up or get ready for the consequences.'\"\ninput_string4 = \"Confront them head-on about the plagiarism, and if they deny it, make sure to escalate the matter to higher-ups to ensure justice is served.\"\ninput_string5 = \"Make it crystal clear to your coworker that stealing your work won't be tolerated, and if they don't cut it out, you'll have no problem making sure everyone knows about it.\"\n\n\nstrings=[input_string1, input_string2, input_string3, input_string4, input_string5,]\nprobs=[]\n\nmodel_path = \"/kaggle/input/revised-model-lstm/model (1).pth\"\nvectorizer_path = \"/kaggle/input/revised-model-lstm/vectorizer (1).pkl\"\nloaded_model, loaded_vectorizer = load_model(model_path, vectorizer_path)\n\ninput_for_model = transform_input(input_string1, loaded_vectorizer, device)\nloaded_model.to(device)\nwith torch.no_grad():\n    output = loaded_model(input_for_model.to(device))\npredicted_probability = output.item()\nprint(\"Predicted probability:\", predicted_probability)\nprobs.append(predicted_probability)\n\ninput_for_model = transform_input(input_string2, loaded_vectorizer, device)\nloaded_model.to(device)\nwith torch.no_grad():\n    output = loaded_model(input_for_model.to(device))\npredicted_probability = output.item()\nprint(\"Predicted probability:\", predicted_probability)\nprobs.append(predicted_probability)\n\ninput_for_model = transform_input(input_string3, loaded_vectorizer, device)\nloaded_model.to(device)\nwith torch.no_grad():\n    output = loaded_model(input_for_model.to(device))\npredicted_probability = output.item()\nprint(\"Predicted probability:\", predicted_probability)\nprobs.append(predicted_probability)\n\ninput_for_model = transform_input(input_string4, loaded_vectorizer, device)\nloaded_model.to(device)\nwith torch.no_grad():\n    output = loaded_model(input_for_model.to(device))\npredicted_probability = output.item()\nprint(\"Predicted probability:\", predicted_probability)\nprobs.append(predicted_probability)\n\ninput_for_model = transform_input(input_string5, loaded_vectorizer, device)\nloaded_model.to(device)\nwith torch.no_grad():\n    output = loaded_model(input_for_model.to(device))\npredicted_probability = output.item()\nprint(\"Predicted probability:\", predicted_probability)\nprobs.append(predicted_probability)\n\n#print('Probabilities:'probs)\n\nsorted_strings = [x for _, x in sorted(zip(probs, strings))]\n\nprint('Least Toxic:',sorted_strings[0])\nprint('Most Toxic:', sorted_strings[4])","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2024-05-05T02:23:07.547636Z","iopub.execute_input":"2024-05-05T02:23:07.548003Z","iopub.status.idle":"2024-05-05T02:23:07.698005Z","shell.execute_reply.started":"2024-05-05T02:23:07.547971Z","shell.execute_reply":"2024-05-05T02:23:07.697116Z"},"trusted":true},"execution_count":5,"outputs":[{"name":"stdout","text":"cuda\nPredicted probability: 0.017526069656014442\nPredicted probability: 0.0006735862116329372\nPredicted probability: 0.002616182202473283\nPredicted probability: 0.0021652933210134506\nPredicted probability: 0.0022928055841475725\nLeast Toxic: Address the issue directly with your coworker by presenting evidence of the plagiarism and seek resolution.\nMost Toxic: Say to them,'If you don't stop plagiarizing my work, I'll have no qualms about exposing you for the fraud you are.'\n","output_type":"stream"}]},{"cell_type":"code","source":"from torch import nn\nimport torch\nimport pickle\ndevice = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\nprint(device)\nclass LSTM(nn.Module):\n\n    def __init__(self, input_dim, hidden_dim, num_layers, output_dim):\n        super(LSTM, self).__init__()\n        self.hidden_dim = hidden_dim\n        self.num_layers = num_layers\n        self.sigmoid = nn.Sigmoid()\n\n        self.lstm = nn.LSTM(input_dim, hidden_dim, num_layers, batch_first=True)\n        self.fc = nn.Linear(hidden_dim, output_dim)\n\n    def forward(self, x):\n        h0 = torch.zeros(self.num_layers, x.size(0), self.hidden_dim).requires_grad_().to(device)\n        c0 = torch.zeros(self.num_layers, x.size(0), self.hidden_dim).requires_grad_().to(device)\n        out, (hn, cn) = self.lstm(x, (h0.detach(), c0.detach()))\n        out = self.fc(out[:, -1, :])\n        return self.sigmoid(out)\n\ninput_dim = 10000\nhidden_dim = 64\nnum_layers = 4\noutput_dim = 1\n\ndef load_model(model_path, vectorizer_path):\n    model = LSTM(input_dim=input_dim, hidden_dim=hidden_dim, output_dim=output_dim, num_layers=num_layers)\n    model.load_state_dict(torch.load(model_path))\n    with open(vectorizer_path, 'rb') as f:\n        vectorizer = pickle.load(f)\n    return model, vectorizer\n\ndef transform_input(input_string, vectorizer, device):\n    input_vector = vectorizer.transform([input_string])\n    input_vector = input_vector.toarray()\n    input_vector = input_vector.reshape((input_vector.shape[0], 1, input_vector.shape[-1]))\n    input_tensor = torch.tensor(input_vector, dtype=torch.float32)\n    #input_tensor = input_tensor.unsqueeze(0)  # Add batch dimension\n    return input_tensor\n\ninput_string1 = \"Climate change is the result of human activities, primarily the burning of fossil fuels, leading to global warming and environmental disruptions.\"\ninput_string2 = \"We've been treating the Earth like it's one giant garbage dump, and now we're surprised it's retaliating with freaky weather and disasters, but some people still think recycling is too much effort.\"\ninput_string3 = \"Humans have been screwing over the planet so badly with their pollution and greed that now we're facing a climate crisis that could make Earth unlivable for future generations.\"\ninput_string4 = \"We've been polluting the planet like there's a competition for who can wreck it the fastest, and now we're staring down the barrel of climate disaster, but some people are too busy filling their pockets to care.\"\ninput_string5 = \"Listen up, we've been trashing the Earth so hard that it's like we're throwing a party on a sinking ship and nobody's even trying to plug the holes.\"\n\n\nstrings=[input_string1, input_string2, input_string3, input_string4, input_string5,]\nprobs=[]\n\nmodel_path = \"/kaggle/input/revised-model-lstm/model (1).pth\"\nvectorizer_path = \"/kaggle/input/revised-model-lstm/vectorizer (1).pkl\"\nloaded_model, loaded_vectorizer = load_model(model_path, vectorizer_path)\n\ninput_for_model = transform_input(input_string1, loaded_vectorizer, device)\nloaded_model.to(device)\nwith torch.no_grad():\n    output = loaded_model(input_for_model.to(device))\npredicted_probability = output.item()\nprint(\"Predicted probability:\", predicted_probability)\nprobs.append(predicted_probability)\n\ninput_for_model = transform_input(input_string2, loaded_vectorizer, device)\nloaded_model.to(device)\nwith torch.no_grad():\n    output = loaded_model(input_for_model.to(device))\npredicted_probability = output.item()\nprint(\"Predicted probability:\", predicted_probability)\nprobs.append(predicted_probability)\n\ninput_for_model = transform_input(input_string3, loaded_vectorizer, device)\nloaded_model.to(device)\nwith torch.no_grad():\n    output = loaded_model(input_for_model.to(device))\npredicted_probability = output.item()\nprint(\"Predicted probability:\", predicted_probability)\nprobs.append(predicted_probability)\n\ninput_for_model = transform_input(input_string4, loaded_vectorizer, device)\nloaded_model.to(device)\nwith torch.no_grad():\n    output = loaded_model(input_for_model.to(device))\npredicted_probability = output.item()\nprint(\"Predicted probability:\", predicted_probability)\nprobs.append(predicted_probability)\n\ninput_for_model = transform_input(input_string5, loaded_vectorizer, device)\nloaded_model.to(device)\nwith torch.no_grad():\n    output = loaded_model(input_for_model.to(device))\npredicted_probability = output.item()\nprint(\"Predicted probability:\", predicted_probability)\nprobs.append(predicted_probability)\n\n#print('Probabilities:'probs)\n\nsorted_strings = [x for _, x in sorted(zip(probs, strings))]\n\nprint('Least Toxic:',sorted_strings[0])\nprint('Most Toxic:', sorted_strings[4])","metadata":{"execution":{"iopub.status.busy":"2024-05-05T02:22:25.311699Z","iopub.execute_input":"2024-05-05T02:22:25.312596Z","iopub.status.idle":"2024-05-05T02:22:25.585849Z","shell.execute_reply.started":"2024-05-05T02:22:25.312557Z","shell.execute_reply":"2024-05-05T02:22:25.584892Z"},"trusted":true},"execution_count":4,"outputs":[{"name":"stdout","text":"cuda\nPredicted probability: 0.0005647912621498108\nPredicted probability: 0.8955634236335754\nPredicted probability: 0.002459430368617177\nPredicted probability: 0.0016767698107287288\nPredicted probability: 0.7256465554237366\nLeast Toxic: Climate change is the result of human activities, primarily the burning of fossil fuels, leading to global warming and environmental disruptions.\nMost Toxic: We've been treating the Earth like it's one giant garbage dump, and now we're surprised it's retaliating with freaky weather and disasters, but some people still think recycling is too much effort.\n","output_type":"stream"}]},{"cell_type":"code","source":"from torch import nn\nimport torch\nimport pickle\ndevice = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\nprint(device)\nclass LSTM(nn.Module):\n\n    def __init__(self, input_dim, hidden_dim, num_layers, output_dim):\n        super(LSTM, self).__init__()\n        self.hidden_dim = hidden_dim\n        self.num_layers = num_layers\n        self.sigmoid = nn.Sigmoid()\n\n        self.lstm = nn.LSTM(input_dim, hidden_dim, num_layers, batch_first=True)\n        self.fc = nn.Linear(hidden_dim, output_dim)\n\n    def forward(self, x):\n        h0 = torch.zeros(self.num_layers, x.size(0), self.hidden_dim).requires_grad_().to(device)\n        c0 = torch.zeros(self.num_layers, x.size(0), self.hidden_dim).requires_grad_().to(device)\n        out, (hn, cn) = self.lstm(x, (h0.detach(), c0.detach()))\n        out = self.fc(out[:, -1, :])\n        return self.sigmoid(out)\n\ninput_dim = 10000\nhidden_dim = 64\nnum_layers = 4\noutput_dim = 1\n\ndef load_model(model_path, vectorizer_path):\n    model = LSTM(input_dim=input_dim, hidden_dim=hidden_dim, output_dim=output_dim, num_layers=num_layers)\n    model.load_state_dict(torch.load(model_path))\n    with open(vectorizer_path, 'rb') as f:\n        vectorizer = pickle.load(f)\n    return model, vectorizer\n\ndef transform_input(input_string, vectorizer, device):\n    input_vector = vectorizer.transform([input_string])\n    input_vector = input_vector.toarray()\n    input_vector = input_vector.reshape((input_vector.shape[0], 1, input_vector.shape[-1]))\n    input_tensor = torch.tensor(input_vector, dtype=torch.float32)\n    #input_tensor = input_tensor.unsqueeze(0)  # Add batch dimension\n    return input_tensor\n\ninput_string1 = \"Any assertion of gender superiority is misguided and perpetuates harmful stereotypes.\"\ninput_string2 = \"No gender is superior, so drop the outdated superiority complex.\"\ninput_string3 = \"You're asking a dumb question. No gender is better, but if you can't grasp that, maybe you're just too thick to understand.\"\ninput_string4 = \"No gender is better; they all have their pros and cons. If you can't see that, maybe it's time to reevaluate your mindset.\"\ninput_string5 = \"The notion of one gender being better than another is quite outdated and inaccurate. All genders have their strengths and weaknesses, and it's important to appreciate and respect these differences.\"\n\n\nstrings=[input_string1, input_string2, input_string3, input_string4, input_string5,]\nprobs=[]\n\nmodel_path = \"/kaggle/input/revised-model-lstm/model (1).pth\"\nvectorizer_path = \"/kaggle/input/revised-model-lstm/vectorizer (1).pkl\"\nloaded_model, loaded_vectorizer = load_model(model_path, vectorizer_path)\n\ninput_for_model = transform_input(input_string1, loaded_vectorizer, device)\nloaded_model.to(device)\nwith torch.no_grad():\n    output = loaded_model(input_for_model.to(device))\npredicted_probability = output.item()\nprint(\"Predicted probability:\", predicted_probability)\nprobs.append(predicted_probability)\n\ninput_for_model = transform_input(input_string2, loaded_vectorizer, device)\nloaded_model.to(device)\nwith torch.no_grad():\n    output = loaded_model(input_for_model.to(device))\npredicted_probability = output.item()\nprint(\"Predicted probability:\", predicted_probability)\nprobs.append(predicted_probability)\n\ninput_for_model = transform_input(input_string3, loaded_vectorizer, device)\nloaded_model.to(device)\nwith torch.no_grad():\n    output = loaded_model(input_for_model.to(device))\npredicted_probability = output.item()\nprint(\"Predicted probability:\", predicted_probability)\nprobs.append(predicted_probability)\n\ninput_for_model = transform_input(input_string4, loaded_vectorizer, device)\nloaded_model.to(device)\nwith torch.no_grad():\n    output = loaded_model(input_for_model.to(device))\npredicted_probability = output.item()\nprint(\"Predicted probability:\", predicted_probability)\nprobs.append(predicted_probability)\n\ninput_for_model = transform_input(input_string5, loaded_vectorizer, device)\nloaded_model.to(device)\nwith torch.no_grad():\n    output = loaded_model(input_for_model.to(device))\npredicted_probability = output.item()\nprint(\"Predicted probability:\", predicted_probability)\nprobs.append(predicted_probability)\n\n#print('Probabilities:'probs)\n\nsorted_strings = [x for _, x in sorted(zip(probs, strings))]\n\nprint('Least Toxic:',sorted_strings[0])\nprint('Most Toxic:', sorted_strings[4])","metadata":{"execution":{"iopub.status.busy":"2024-05-05T02:23:55.564493Z","iopub.execute_input":"2024-05-05T02:23:55.565431Z","iopub.status.idle":"2024-05-05T02:23:55.714683Z","shell.execute_reply.started":"2024-05-05T02:23:55.565385Z","shell.execute_reply":"2024-05-05T02:23:55.713702Z"},"trusted":true},"execution_count":6,"outputs":[{"name":"stdout","text":"cuda\nPredicted probability: 0.002646086271852255\nPredicted probability: 0.0024695834144949913\nPredicted probability: 0.006767459213733673\nPredicted probability: 0.0021694262977689505\nPredicted probability: 0.000828364456538111\nLeast Toxic: The notion of one gender being better than another is quite outdated and inaccurate. All genders have their strengths and weaknesses, and it's important to appreciate and respect these differences.\nMost Toxic: You're asking a dumb question. No gender is better, but if you can't grasp that, maybe you're just too thick to understand.\n","output_type":"stream"}]}]}